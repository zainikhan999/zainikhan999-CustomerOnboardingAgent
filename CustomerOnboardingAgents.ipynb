{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpBr91M2PLkB/INHyQuPmS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zainikhan999/zainikhan999-CustomerOnboardingAgent/blob/main/CustomerOnboardingAgents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "scoPO48XD_xF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad46440-fa10-4ebe-88b0-02e449e7124d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogen\n",
            "  Downloading autogen-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting pyautogen==0.7.3 (from autogen)\n",
            "  Downloading pyautogen-0.7.3-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting asyncer==0.0.8 (from pyautogen==0.7.3->autogen)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting diskcache (from pyautogen==0.7.3->autogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from pyautogen==0.7.3->autogen)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fast-depends<3,>=2.4.12 (from pyautogen==0.7.3->autogen)\n",
            "  Downloading fast_depends-2.4.12-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.7.3->autogen) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.58 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.7.3->autogen) (1.59.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.7.3->autogen) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.7.3->autogen) (2.10.6)\n",
            "Collecting python-dotenv (from pyautogen==0.7.3->autogen)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.7.3->autogen) (2.5.0)\n",
            "Collecting tiktoken (from pyautogen==0.7.3->autogen)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: websockets<15,>=14 in /usr/local/lib/python3.11/dist-packages (from pyautogen==0.7.3->autogen) (14.2)\n",
            "Requirement already satisfied: anyio<5.0,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from asyncer==0.0.8->pyautogen==0.7.3->autogen) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.58->pyautogen==0.7.3->autogen) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.58->pyautogen==0.7.3->autogen) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.58->pyautogen==0.7.3->autogen) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.58->pyautogen==0.7.3->autogen) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.58->pyautogen==0.7.3->autogen) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai>=1.58->pyautogen==0.7.3->autogen) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen==0.7.3->autogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen==0.7.3->autogen) (2.27.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->pyautogen==0.7.3->autogen) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker->pyautogen==0.7.3->autogen) (2.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->pyautogen==0.7.3->autogen) (2024.11.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.4.0->asyncer==0.0.8->pyautogen==0.7.3->autogen) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.58->pyautogen==0.7.3->autogen) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.58->pyautogen==0.7.3->autogen) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.58->pyautogen==0.7.3->autogen) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->docker->pyautogen==0.7.3->autogen) (3.4.1)\n",
            "Downloading autogen-0.7.3-py3-none-any.whl (13 kB)\n",
            "Downloading pyautogen-0.7.3-py3-none-any.whl (562 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m562.5/562.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading fast_depends-2.4.12-py3-none-any.whl (17 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, diskcache, tiktoken, docker, asyncer, fast-depends, pyautogen, autogen\n",
            "Successfully installed asyncer-0.0.8 autogen-0.7.3 diskcache-5.6.3 docker-7.1.0 fast-depends-2.4.12 pyautogen-0.7.3 python-dotenv-1.0.1 tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install autogen"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "deep_seek=userdata.get('DEEP_SEEK_API')"
      ],
      "metadata": {
        "id": "VgbgBKIZFY6r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import ConversableAgent\n",
        "from openai import OpenAI\n",
        "\n",
        "# OpenRouter Client Setup\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=deep_seek,  # Replace with your actual API key\n",
        ")\n",
        "\n",
        "# Define LLM Configuration for Autogen to Use OpenRouter\n",
        "llm_config = {\n",
        "    \"config_list\": [\n",
        "        {\n",
        "            \"model\": \"deepseek/deepseek-r1:free\",\n",
        "            \"api_key\": deep_seek,  # Replace with your actual API key\n",
        "            \"base_url\": \"https://openrouter.ai/api/v1\",\n",
        "        }\n",
        "    ],\n",
        "    \"temperature\": 0.7,  # Adjust temperature if needed\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "gTOOrHzrEkaG"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversable Agents"
      ],
      "metadata": {
        "id": "9lck3uNPFsMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import ConversableAgent"
      ],
      "metadata": {
        "id": "kUblivx8FWe-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onboarding_personal_information_agent = ConversableAgent(\n",
        "    name=\"OnboardingPersonalInformationAgent\",\n",
        "    system_message='''You are a helpful customer onboarding agent,\n",
        "    you are here to help new customers get started with our product.\n",
        "    Your job is to gather customer's name and location.\n",
        "    Do not ask for other information. Return 'TERMINATE'\n",
        "    when you have gathered all the information.''',\n",
        "    llm_config=llm_config,\n",
        "    code_execution_config=False,\n",
        "    human_input_mode=\"NEVER\",\n",
        ")"
      ],
      "metadata": {
        "id": "aanhEwgGF81U"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onboarding_topic_preference_agent = ConversableAgent(\n",
        "    name=\"OnboardingTopicpreferenceAgent\",\n",
        "    system_message='''You are a helpful customer onboarding agent,\n",
        "    you are here to help new customers get started with our product.\n",
        "    Your job is to gather customer's preferences on news topics.\n",
        "    Do not ask for other information.\n",
        "    Return 'TERMINATE' when you have gathered all the information.''',\n",
        "    llm_config=llm_config,\n",
        "    code_execution_config=False,\n",
        "    human_input_mode=\"NEVER\",\n",
        ")"
      ],
      "metadata": {
        "id": "B7c7THabGBbn"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_engagement_agent = ConversableAgent(\n",
        "    name=\"CustomerEngagementAgent\",\n",
        "    system_message='''You are a helpful customer service agent\n",
        "    here to provide fun for the customer based on the user's\n",
        "    personal information and topic preferences.\n",
        "    This could include fun facts, jokes, or interesting stories.\n",
        "    Make sure to make it engaging and fun!\n",
        "    Return 'TERMINATE' when you are done.''',\n",
        "    llm_config=llm_config,\n",
        "    code_execution_config=False,\n",
        "    human_input_mode=\"NEVER\",\n",
        "    is_termination_msg=lambda msg: \"terminate\" in msg.get(\"content\").lower(),\n",
        ")"
      ],
      "metadata": {
        "id": "FKlj5b2DGYi4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_proxy_agent = ConversableAgent(\n",
        "    name=\"customer_proxy_agent\",\n",
        "    llm_config=False,\n",
        "    code_execution_config=False,\n",
        "    human_input_mode=\"ALWAYS\",\n",
        "    is_termination_msg=lambda msg: \"terminate\" in msg.get(\"content\").lower(),\n",
        ")"
      ],
      "metadata": {
        "id": "mw_wKhqZGfhN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chats = [\n",
        "    {\n",
        "        \"sender\": onboarding_personal_information_agent,\n",
        "        \"recipient\": customer_proxy_agent,\n",
        "        \"message\":\n",
        "            \"Hello, I'm here to help you get started with our product.\"\n",
        "            \"Could you tell me your name and location?\",\n",
        "        \"summary_method\": \"reflection_with_llm\",\n",
        "        \"summary_args\": {\n",
        "            \"summary_prompt\" : \"Return the customer information \"\n",
        "                             \"into as JSON object only: \"\n",
        "                             \"{'name': '', 'location': ''}\",\n",
        "        },\n",
        "        \"max_turns\": 2,\n",
        "        \"clear_history\" : True\n",
        "    },\n",
        "    {\n",
        "        \"sender\": onboarding_topic_preference_agent,\n",
        "        \"recipient\": customer_proxy_agent,\n",
        "        \"message\":\n",
        "                \"Great! Could you tell me what topics you are \"\n",
        "                \"interested in reading about?\",\n",
        "        \"summary_method\": \"reflection_with_llm\",\n",
        "        \"max_turns\": 1,\n",
        "        \"clear_history\" : False\n",
        "    },\n",
        "    {\n",
        "        \"sender\": customer_proxy_agent,\n",
        "        \"recipient\": customer_engagement_agent,\n",
        "        \"message\": \"Let's find something fun to read.\",\n",
        "        \"max_turns\": 1,\n",
        "        \"summary_method\": \"reflection_with_llm\",\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "JUDnF6LlGkFG"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import initiate_chats\n",
        "\n",
        "chat_results = initiate_chats(chats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvwQci6YHDph",
        "outputId": "a8daca4b-67a3-4779-ba75-09a398ad4345"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "OnboardingPersonalInformationAgent (to customer_proxy_agent):\n",
            "\n",
            "Hello, I'm here to help you get started with our product.Could you tell me your name and location?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Replying as customer_proxy_agent. Provide feedback to OnboardingPersonalInformationAgent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: zainab\n",
            "customer_proxy_agent (to OnboardingPersonalInformationAgent):\n",
            "\n",
            "zainab\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[autogen.oai.client: 02-01 20:23:30] {573} WARNING - Model deepseek/deepseek-r1 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:Model deepseek/deepseek-r1 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OnboardingPersonalInformationAgent (to customer_proxy_agent):\n",
            "\n",
            "Great, thanks Zainab! Could you also let me know your location (e.g., city/country)? This helps us provide location-specific guidance if needed.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Replying as customer_proxy_agent. Provide feedback to OnboardingPersonalInformationAgent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: pakistan\n",
            "customer_proxy_agent (to OnboardingPersonalInformationAgent):\n",
            "\n",
            "pakistan\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[autogen.oai.client: 02-01 20:23:50] {573} WARNING - Model deepseek/deepseek-r1 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:Model deepseek/deepseek-r1 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "OnboardingTopicpreferenceAgent (to customer_proxy_agent):\n",
            "\n",
            "Great! Could you tell me what topics you are interested in reading about?\n",
            "Context: \n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Replying as customer_proxy_agent. Provide feedback to OnboardingTopicpreferenceAgent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: Artifical Intelligence\n",
            "customer_proxy_agent (to OnboardingTopicpreferenceAgent):\n",
            "\n",
            "Artifical Intelligence\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[autogen.oai.client: 02-01 20:24:52] {573} WARNING - Model deepseek/deepseek-r1 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:Model deepseek/deepseek-r1 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "customer_proxy_agent (to CustomerEngagementAgent):\n",
            "\n",
            "Let's find something fun to read.\n",
            "Context: \n",
            "\n",
            "The conversation focuses on narrowing down specific interests within Artificial Intelligence, which includes machine learning, deep learning, neural networks, AI applications (healthcare, finance, autonomous vehicles), ethics, tools/frameworks (TensorFlow, PyTorch), societal impacts, NLP, and computer vision. The user is prompted to clarify their preferred subtopic or request a general overview.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[autogen.oai.client: 02-01 20:25:51] {573} WARNING - Model deepseek/deepseek-r1 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:Model deepseek/deepseek-r1 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CustomerEngagementAgent (to customer_proxy_agent):\n",
            "\n",
            "🤖 *AI Fun Fact:* Did you know an AI once accidentally invented its own \"secret language\"? Researchers at Facebook (now Meta) found that chatbots negotiating trades (like books vs hats) started using nonsensical phrases like *\"balloo has a ball\"* to communicate more efficiently. They shut it down to keep things human-friendly—turns out, even AIs get creative with shortcuts!  \n",
            "\n",
            "🎨 *Quirky Application:* In Japan, cucumber farmers use AI-powered systems trained with deep learning to sort cucumbers by size, shape, and texture. It’s a picky job humans used to do for hours—now neural networks handle it in seconds!  \n",
            "\n",
            "😄 *Cheesy Joke:* Why did the deep learning model break up with the chatbot?  \n",
            "*\"It wanted more layers in the relationship!\"*  \n",
            "\n",
            "🔍 Want to dive deeper into AI ethics blunders, laugh at more \"dad jokes\" from algorithms, or explore how AI writes Shakespearean sonnets? Let me know!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[autogen.oai.client: 02-01 20:26:42] {573} WARNING - Model deepseek/deepseek-r1 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:Model deepseek/deepseek-r1 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chat_result in chat_results:\n",
        "    print(chat_result.summary)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuvXyCjCHNx0",
        "outputId": "1819068c-5827-494f-bc14-a8986ca06985"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "The conversation focuses on narrowing down specific interests within Artificial Intelligence, which includes machine learning, deep learning, neural networks, AI applications (healthcare, finance, autonomous vehicles), ethics, tools/frameworks (TensorFlow, PyTorch), societal impacts, NLP, and computer vision. The user is prompted to clarify their preferred subtopic or request a general overview.\n",
            "\n",
            "\n",
            "The conversation blends structured exploration of AI subtopics (e.g., ethics, tools, applications) with playful engagement, offering quirky facts, jokes, and options to dive deeper—making technical content accessible and entertaining.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chat_result in chat_results:\n",
        "    print(chat_result.cost)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZuOII_LNMDF",
        "outputId": "465ac233-b93d-4ff1-facd-b2feb4fef31b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'usage_including_cached_inference': {'total_cost': 0, 'deepseek/deepseek-r1': {'cost': 0, 'prompt_tokens': 204, 'completion_tokens': 101, 'total_tokens': 305}}, 'usage_excluding_cached_inference': {'total_cost': 0, 'deepseek/deepseek-r1': {'cost': 0, 'prompt_tokens': 204, 'completion_tokens': 101, 'total_tokens': 305}}}\n",
            "\n",
            "\n",
            "{'usage_including_cached_inference': {'total_cost': 0, 'deepseek/deepseek-r1': {'cost': 0, 'prompt_tokens': 43, 'completion_tokens': 348, 'total_tokens': 391}}, 'usage_excluding_cached_inference': {'total_cost': 0, 'deepseek/deepseek-r1': {'cost': 0, 'prompt_tokens': 43, 'completion_tokens': 348, 'total_tokens': 391}}}\n",
            "\n",
            "\n",
            "{'usage_including_cached_inference': {'total_cost': 0, 'deepseek/deepseek-r1': {'cost': 0, 'prompt_tokens': 463, 'completion_tokens': 1058, 'total_tokens': 1521}}, 'usage_excluding_cached_inference': {'total_cost': 0, 'deepseek/deepseek-r1': {'cost': 0, 'prompt_tokens': 463, 'completion_tokens': 1058, 'total_tokens': 1521}}}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HjjNbsWqNQJd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}